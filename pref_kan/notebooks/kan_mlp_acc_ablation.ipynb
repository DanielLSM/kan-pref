{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/dev/kan-pref/pref_kan/notebooks\n",
      "created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/software/anaconda3/envs/kan/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniel/software/anaconda3/envs/kan/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206809/1073302580.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  tensor_o1 = torch.Tensor(traj1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:MLP, Epoch 50 , Training loss (for one batch) at step 4: 60.4210, Accuracy 0.7000\n",
      "Seen so far: 160 samples\n",
      "Model:MLP, Epoch 100 , Training loss (for one batch) at step 4: 58.9215, Accuracy 0.7500\n",
      "Seen so far: 160 samples\n",
      "Model:MLP, Epoch 150 , Training loss (for one batch) at step 4: 58.0886, Accuracy 0.6500\n",
      "Seen so far: 160 samples\n",
      "Model:MLP, Epoch 200 , Training loss (for one batch) at step 4: 59.3381, Accuracy 0.6500\n",
      "Seen so far: 160 samples\n",
      "Model:KAN, Epoch 50 , Training loss (for one batch) at step 4: 58.6512, Accuracy 0.7000\n",
      "Seen: 160 samples\n",
      "Model:KAN, Epoch 100 , Training loss (for one batch) at step 4: 57.6346, Accuracy 0.8500\n",
      "Seen: 160 samples\n",
      "Model:KAN, Epoch 150 , Training loss (for one batch) at step 4: 59.4171, Accuracy 0.7000\n",
      "Seen: 160 samples\n",
      "Model:KAN, Epoch 200 , Training loss (for one batch) at step 4: 60.2418, Accuracy 0.6500\n",
      "Seen: 160 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle as pkl\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pref_kan.oracle import HumanCritic\n",
    "from pref_kan.kan_oracle import HumanCritic as HumanCriticKan\n",
    "\n",
    "def load_pickle(name):\n",
    "    with open(name + \".pkl\", 'rb') as handle:\n",
    "        return pkl.load(handle)\n",
    "\n",
    "def generate_data_for_training_with_critical_points(queries):\n",
    "    queries = np.array(queries, dtype=object)\n",
    "    o1, o2, prefs = queries[:, 0, 0], queries[:, 1, 0], queries[:, 2]\n",
    "    o1 = [np.stack(segments) for segments in o1]\n",
    "    o2 = [np.stack(segments) for segments in o2]\n",
    "    prefs = np.asarray(prefs).astype('float32')\n",
    "    return o1, o2, prefs\n",
    "\n",
    "def create_dataloader(pairs):\n",
    "\n",
    "    traj1, traj2, prefs = generate_data_for_training_with_critical_points(pairs)\n",
    "    tensor_o1 = torch.Tensor(traj1)\n",
    "    tensor_o2 = torch.Tensor(traj2)\n",
    "    tensor_prefs = torch.Tensor(prefs)\n",
    "    my_dataset = TensorDataset(tensor_o1, tensor_o2, tensor_prefs)\n",
    "    my_dataloader = DataLoader(my_dataset, batch_size=128, shuffle=True)\n",
    "    return my_dataloader\n",
    "\n",
    "def test_accuracy(dataset, oracle):\n",
    "    total_params = sum(p.numel() for p in oracle.reward_model.parameters())\n",
    "    for step, (o1, o2, prefs) in enumerate(dataset):\n",
    "            o1 = o1.to('cpu')  # Move input tensors to the device\n",
    "            o2 = o2.to('cpu')\n",
    "            prefs = prefs.to('cpu')\n",
    "            o1_unrolled = torch.reshape(o1, [-1, oracle.obs_size[0] + oracle.action_size])\n",
    "            o2_unrolled = torch.reshape(o2, [-1, oracle.obs_size[0] + oracle.action_size])\n",
    "            r1_unrolled = oracle.reward_model(o1_unrolled)\n",
    "            r2_unrolled = oracle.reward_model(o2_unrolled)\n",
    "\n",
    "            r1_rolled = torch.reshape(r1_unrolled, o1.shape[0:2])\n",
    "            r2_rolled = torch.reshape(r2_unrolled, o2.shape[0:2])\n",
    "\n",
    "            rs1 = torch.sum(r1_rolled, dim=1)\n",
    "            rs2 = torch.sum(r2_rolled, dim=1)\n",
    "            rss = torch.stack([rs1, rs2])\n",
    "            rss = torch.t(rss)\n",
    "\n",
    "            preds = torch.softmax(rss, dim=0)\n",
    "            preds_correct = torch.eq(torch.argmax(prefs, 1), torch.argmax(preds, 1)).type(torch.float32)\n",
    "            accuracy = torch.mean(preds_correct)\n",
    "\n",
    "\n",
    "    return accuracy.item(), total_params        \n",
    "\n",
    "env_id = \"HalfCheetah-v3\"\n",
    "data_set_name = \"/../data/cheetah/rl_zoopairs_cheetahbuffer\"\n",
    "sizes_data_set_name = \"/../data/cheetah/rl_zoopairs_size_cheetahbuffer\"\n",
    "path = os.getcwd()\n",
    "\n",
    "pairs = load_pickle(path + data_set_name)\n",
    "pairs_size = load_pickle(path + sizes_data_set_name)\n",
    "\n",
    "env = gym.make(env_id)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.shape[0]\n",
    "oracle = HumanCritic(state_size, action_size,hidden_sizes=(64,64), training_epochs=200,seed=123)\n",
    "oracle_kan = HumanCriticKan(state_size,action_size,hidden_sizes=(6,2), training_epochs=200,seed=123)\n",
    "\n",
    "training_pairs = [pairs[idx] for idx in range(pairs_size-10)]\n",
    "training_dataloader = create_dataloader(training_pairs)\n",
    "\n",
    "oracle.train_dataset(training_dataloader, {})\n",
    "oracle_kan.train_dataset(training_dataloader, {})\n",
    "\n",
    "\n",
    "test_pairs = [pairs[idx] for idx in range(pairs_size-10, pairs_size)]\n",
    "test_dataloader = create_dataloader(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [1,5,123,1234]\n",
    "accuracy_mlp_list, accuracy_kan_list = [], []\n",
    "for seed in seed_list:\n",
    "    oracle = HumanCritic(state_size, action_size,hidden_sizes=(64,64), training_epochs=200,seed=seed)\n",
    "    oracle_kan = HumanCriticKan(state_size,action_size,hidden_sizes=(6,2), training_epochs=200,seed=seed)\n",
    "    accuracy_mlp, params_mlp = test_accuracy(test_dataloader, oracle)\n",
    "    accuracy_kan, params_kan = test_accuracy(test_dataloader, oracle_kan)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
