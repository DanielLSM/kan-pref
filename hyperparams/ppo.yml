atari:
  env_wrapper:
    - stable_baselines3.common.atari_wrappers.AtariWrapper
  frame_stack: 4
  policy: 'CnnPolicy'
  n_envs: 8
  n_steps: 128
  n_epochs: 4
  batch_size: 256
  n_timesteps: !!float 1e7
  learning_rate: lin_2.5e-4
  clip_range: lin_0.1
  vf_coef: 0.5
  ent_coef: 0.01

HalfCheetah-v3:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 64
  n_steps: 512
  gamma: 0.98
  learning_rate: 2.0633e-05
  ent_coef: 0.000401762
  clip_range: 0.1
  n_epochs: 20
  gae_lambda: 0.92
  max_grad_norm: 0.8
  vf_coef: 0.58096
  policy_kwargs: "dict(
                    log_std_init=-2,
                    ortho_init=False,
                    activation_fn=nn.ReLU,
                    net_arch=[dict(pi=[256, 256], vf=[256, 256])]
                  )"
  pref_learning:
    active: true
    human_critic:
      maximum_segment_buffer: 1000000
      maximum_preference_buffer: 2100
      training_epochs: 10
      batch_size: 128
      hidden_sizes: [ 256, 256, 256 ]
      traj_k_lenght: 50
      weight_decay: 0.001
      learning_rate: 0.0003
      regularize: false
      custom_oracle: false
      loss_type: reward
    wrapper:
      - utils.wrappers.HumanReward:
          human_critic: true
    callback:
      - pref.callbacks.UpdateRewardFunctionCriticalPoint:
          human_critic: true
          use_env_name: true
          n_queries: 50
          initial_reward_estimation_epochs: 200
          reward_training_epochs: 50
          truth: 90
          traj_length: 50
          smallest_rew_threshold: -0.3
          largest_rew_threshold: 0.3
          n_initial_queries: 400
          max_queries: 1400
          every_n_timestep: 20000

Walker2d-v3:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  #n_timesteps: !!float 9e4
  batch_size: 32
  n_steps: 512
  gamma: 0.99
  learning_rate: 5.05041e-05
  ent_coef: 0.000585045
  clip_range: 0.1
  n_epochs: 20
  gae_lambda: 0.95
  max_grad_norm: 1
  vf_coef: 0.871923
  pref_learning:
    active: true
    human_critic:
      maximum_segment_buffer: 1000000
      maximum_preference_buffer: 1400
      training_epochs: 10
      batch_size: 64
      hidden_sizes: [ 256, 256, 256 ]
      traj_k_lenght: 50
      weight_decay: 0.0
      learning_rate: 0.0003
      regularize: false
      custom_oracle: false
      epsilon: 2
    wrapper:
      - utils.wrappers.HumanReward:
          human_critic: true
    callback:
      - pref.callbacks.UpdateRewardFunctionCriticalPoint:
          human_critic: true
          use_env_name: true
          n_queries: 50
          initial_reward_estimation_epochs: 200
          reward_training_epochs: 50
          truth: 90
          traj_length: 50
          smallest_rew_threshold: -1
          largest_rew_threshold: 1
          n_initial_queries: 200
          max_queries: 700
          every_n_timestep: 20000

drawer-close-v2:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 64
  n_steps: 512
  gamma: 0.98
  learning_rate: 2.0633e-05
  ent_coef: 0.000401762
  clip_range: 0.1
  n_epochs: 20
  gae_lambda: 0.92
  max_grad_norm: 0.8
  vf_coef: 0.58096
  policy_kwargs: "dict(
                    log_std_init=-2,
                    ortho_init=False,
                    activation_fn=nn.ReLU,
                    net_arch=[dict(pi=[256, 256], vf=[256, 256])]
                  )"
  env_wrapper:
    - utils.wrappers.MetaworldStopping
  pref_learning:
   active: true
   human_critic:
     maximum_segment_buffer: 1000000
     maximum_preference_buffer: 2100
     training_epochs: 10
     batch_size: 128
     hidden_sizes: [ 256, 256, 256 ]
     traj_k_lenght: 50
     weight_decay: 0.0
     learning_rate: 0.0003
     regularize: false
     custom_oracle: false
   wrapper:
     - utils.wrappers.HumanReward:
         human_critic: true
   callback:
     - pref.callbacks.UpdateRewardFunctionCriticalPoint:
         human_critic: true
         use_env_name: true
         n_queries: 30
         initial_reward_estimation_epochs: 200
         reward_training_epochs: 50
         truth: 90
         traj_length: 50
         smallest_rew_threshold: 0.28
         largest_rew_threshold: 1.5
         n_initial_queries: 40
         max_queries: 1000
         every_n_timestep: 20000


Reacher-v2:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 32
  n_steps: 512
  gamma: 0.9
  learning_rate: 0.000104019
  ent_coef: 7.52585e-08
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 1.0
  max_grad_norm: 0.9
  vf_coef: 0.950368
  pref_learning: #Untuned
    active: true
    human_critic:
      maximum_segment_buffer: 1000000
      maximum_preference_buffer: 1400
      training_epochs: 10
      batch_size: 64
      hidden_sizes: [ 256, 256, 256 ]
      traj_k_lenght: 50
      weight_decay: 0.0
      learning_rate: 0.0003
      regularize: false
      custom_oracle: false
      epsilon: 0.1
    wrapper:
      - utils.wrappers.HumanReward:
          human_critic: true
    callback:
      - pref.callbacks.UpdateRewardFunctionCriticalPoint:
          human_critic: true
          use_env_name: true
          n_queries: 50
          initial_reward_estimation_epochs: 200
          reward_training_epochs: 50
          truth: 90
          traj_length: 50
          smallest_rew_threshold: -1
          largest_rew_threshold: 1
          n_initial_queries: 200
          max_queries: 700
          every_n_timestep: 20000

button-press-v2:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  n_steps: 2048
  batch_size: 256
  gae_lambda: 0.92
  gamma: 0.98
  n_epochs: 20
  ent_coef: 0.06306330978144302
  learning_rate: 0.001314214466133982
  clip_range: 0.1
  max_grad_norm: 0.8
  vf_coef: 0.35125493931539353
  policy_kwargs: "dict(
                      log_std_init=-2,
                      ortho_init=False,
                      net_arch=[dict(pi=[256, 256], vf=[256, 256])]
                    )"
  env_wrapper:
    - utils.wrappers.MetaworldStopping
  pref_learning:
    active: true
    human_critic:
      maximum_segment_buffer: 1000000
      maximum_preference_buffer: 2100
      training_epochs: 10
      batch_size: 128
      hidden_sizes: [ 256, 256, 256 ]
      traj_k_lenght: 50
      weight_decay: 0.0
      learning_rate: 0.0003
      regularize: false
      custom_oracle: false
    wrapper:
      - utils.wrappers.HumanReward:
          human_critic: true
    callback:
      - pref.callbacks.UpdateRewardFunctionCriticalPoint:
          human_critic: true
          use_env_name: true
          n_queries: 30
          initial_reward_estimation_epochs: 200
          reward_training_epochs: 50
          truth: 90
          traj_length: 50
          smallest_rew_threshold: 0.15
          largest_rew_threshold: 0.7
          n_initial_queries: 40
          max_queries: 1000
          every_n_timestep: 20000

Default:
  n_envs: 10
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  n_steps: 2014
  gae_lambda: 0.95
  gamma: 0.99
  n_epochs: 10
  ent_coef: 0.001
  batch_size: 64
  learning_rate: !!float 1e-4
  clip_range: 0.2